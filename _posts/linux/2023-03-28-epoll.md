---
layout: post
title: "epoll、socket"
category: GNU-linux
date: 2023-03-28 08:00:00 +0800
---

## epoll底层机制

epoll的底层机制使用的是`waitqueue（等待队列）`而不是软中断。

关于`wait queue`：<https://www.cnblogs.com/hueyxu/p/13745029.html>、<https://unix.stackexchange.com/questions/268027/how-does-linux-kernel-find-out-which-process-to-wake-up-during-interrupt-handlin>

等待队列一些函数在`wait.c，wait.h`中实现。初始化等待队列头：init_waitqueue_head()，等待队列头用于实现阻塞和唤醒的相关机制。添加到等待队列实际上就是放在对应的等待队列头后面，等待队列头唤醒时，唤醒队列上的其他task。

唤醒等待队列需要调用`wake_up_*()`相关的函数，这种函数一般是在驱动程序、文件系统、网络子系统、定时器中实现。

以定时器的实现为例：<https://void-star.icu/archives/573>，在process_timeout()中会调用到wake_up_process，这个函数实际上是对try_to_wake_up()的封装，try_to_wake_up()会唤醒对应的进程。

epoll实现的文档实现可以参考：<https://www.jxhs.me/2021/04/08/linux%E5%86%85%E6%A0%B8Epoll-%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/>

关键点总结：

1. 使用`epoll_create()`会创建一个eventpoll的对象。
2. 在我们通过`epoll_ctl()`添加某个fd进行监控时，内核通过调用`ep_insert()`函数创建一个epitem，并将其插入到eventpoll下由epitem组成的红黑树上。
3. `ep_insert()`中还有一个关键的步骤是调用`tfile->f_op->poll(tfile, &epq.pt)`，这是调用当前添加的fd对应的poll()实现。不同类型的fd有不同类型的实现。下面以tcp_poll为例。
4. tcp_poll的内部实现会将epitem放在fd的等待队列中，如果fd的状态发生变化，唤醒对应的进程。

唤醒对应的函数为：`wake_up_*()`，`wake_up_*()`（常见的例如：wake_up_bit()）一般是在驱动或fs目录中实现的，对应下面的第9步？

补充说明：`wake_up_bit()`等函数最终会调用到`kernel/sched/wait.c`的`__wake_up_common()`，在`__wake_up_common()`中，对每个entry调用`curr->func()`，这里的`func`在`init_wait_entry`中被初始化为`autoremove_wake_function`，而`autoremove_wake_function`调用的正是`default_wake_function => try_to_wake_up`。

## todo

需要补充一下定时器和waitqueue之间的关系。两个有关系吗？

## epoll和select的区别

||select|epoll|
|-|-|-|
|性能|随着连接数量的增加，性能急剧下降|连接数量增加时，性能没有明显损耗|
|连接数|最多支持默认1024个连接，超过限制需要重新配置宏编译|无限制|
|内在的处理逻辑|线性轮询|回调callback|
|开发难度|低|高|

poll和select的区别其实不大，poll对select的优化在：（1）没有文件描述符数量的限制（2）文件描述符集合从用户空间拷贝到内核空间的开销较小。

select的时间复杂度是O(n)的，而epoll的时间复杂度是O(1)的。在连接数少且十分活跃的情况下，select的性能其实更好一些。

epoll的水平触发和边缘触发：

||水平触发|边缘触发|
|-|-|-|
|差异|只要有数据可读，epoll_wait都会返回事件|只在数据到达时返回一次|
|其他|默认模式|高速模式|
|注意点|无|必须在读取数据时把数据读完|

## 网络数据的处理流程

ChatGPT怎么说：

1. 数据包到达: 当网卡接收到一个数据包，它会根据预先配置的DMA（Direct Memory Access）将数据直接写入到内存中，并且触发一个硬件中断。
2. 硬件中断: CPU接收到硬件中断信号后，会暂停当前执行的任务，保存其状态，并执行中断服务程序（Interrupt Service Routine, ISR）。在Linux中，每个设备驱动会注册自己的ISR。
3. 中断服务程序: 网卡的ISR会进行一些基本的处理，如确认中断来源、读取状态寄存器、清除中断标志等，然后通常会将实际的数据处理工作推迟到一个软中断或工作队列中去完成，这样可以尽快释放中断处理程序，减少对CPU的占用。
4. NAPI（New API）: 现代Linux网络驱动通常使用NAPI结构来提高性能和减少中断负载。在高负载下，NAPI可以禁用中断，并改用轮询模式来处理网络数据包。
5. 软中断和工作队列: 数据包处理通常在软中断（softirq）或工作队列（workqueue）的上下文中进行。这些机制允许内核延迟处理函数的执行，从而不会阻塞中断服务程序。
6. 内核网络栈处理: 数据包被传递到内核网络栈，这里包括了多层处理，如链路层、网络层（IP层）、传输层（TCP/UDP）等。
7. 套接字缓冲区: 一旦内核确定了数据包属于哪个套接字，它会将数据包放入该套接字的接收缓冲区中。如果缓冲区已满，新的数据包可能会被丢弃，这取决于所使用的协议和套接字的设置。
8. 唤醒等待进程: 如果有用户态的进程正在等待这些数据（例如，通过调用read()或recv()等系统调用），内核会将这些进程从它们的等待队列中唤醒。
9. 系统调用: 用户态的应用程序通过系统调用与内核通信，请求网络数据。常见的系统调用包括read(), recv(), recvfrom(), recvmsg()等。
10. 用户态处理: 用户进程接收到数据后，可以进行相应的处理，如解析应用层协议，处理业务逻辑等。

### NAPI机制

介绍：<https://blog.csdn.net/Rong_Toa/article/details/109401935>

NAPI机制能够降低频繁中断造成的负载和丢包问题。

## socket

<https://www.less-bug.com/posts/in-depth-understanding-of-the-principle-and-use-of-socket/>

### rust的Stream

我们在写C语言的服务端时，通过socket, bind, listen, accept，这个时候总会获取到一个fd。这个fd是随着连接的增加而增加的，并且每一个连接使用的fd不同。通过这个fd我们能够获取到一些凭据信息：SO_PEERCRED。但是事情在rust里面却不一样。我们通过UnixListener::bind()，listener.accept()获取到一个stream，如果通过stream.as_raw_fd()获取fd时，这里获取到都是同一个fd。原因现在不知。如果想要获取凭据信息，我们必须对stream使用nightly的peer_cred()接口。

一个规避的方法就是使用nix提供的封装，用rust的方式写c的那套：socket，bind，listen，accept。
