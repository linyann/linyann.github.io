---
layout: post
date: 2023-12-29 16:00:00 +0800
category: kernel
title: "关于cgroup"
---

## cgroup.c

cgroup.c是linux cgroup框架层的具体代码实现。

## cgroup-v2接口简单介绍

<https://zorrozou.github.io/docs/%E8%AF%A6%E8%A7%A3Cgroup%20V2.html>

## cgroup-v2相比cgroup-v1的变化

<https://www.infoq.cn/article/hbqqfeyqxzhnes5jipqt>

**no internal processes：进程只能绑定到叶子节点，线程不受影响**

## systemd和cgroup

### 如何通过systemd启用cgroup v2

在内核命令行配置参数：`systemd.unified_cgroup_hierarchy=1`。

### 常用的cgroup接口

|接口名称|cgroup版本|说明|
|-|-|-|
|cgroup.controllers|v2|当前cgroup可以配置的子系统|
|cgroup.subtree_control|v2|当前cgroup的子目录默认继承的子系统，subtree_control应该是controllers的子集|
|cgroup.events|v2|如果子cgroup有进程存在，populated为1；否则为0。可以使用inotify监控|
|cgroup.stat|v2|记录cgroup中存活、死亡的子cgroup数量|
|cgroup.max.depth、cgroup.max.descendants|v2|分别表示子cgroup的最大深度、子cgroup的最大数量|
|cgroup.threads|v2|cgroup包含的线程|
|cgroup.pressure|v2|cgroup当前的cpu、io、memory压力|
|cgroup.type|v2|cgroup类型，可能为normal、threaded|
|notify_on_release、release_agent|v1|如果notify_on_release配置为1，则cgroup内进程清空后会调用release_agent脚本|
|cgroup.clone_children|v1|配置为1后，某些子系统的cgroup配置可以继承到子cgroup，例如cpuset的绑核配置|

在切换cgroup-v2后，子系统的cgroup接口有大规模变更。考虑到systemd目前是k8s的默认cgroup驱动，这里也列出各子系统systemd的常用配置。systemd原来面向cgroup-v1的一些`systemd.resource-control`配置已在systemd-252版本标记为弃用，用户的相关配置会被systemd自动转换为对应的cgroup-v2配置。

### cpu子系统接口

|接口名称|systemd配置|cgroup版本|说明|
|-|-|-|-|
|cpu.cfs_period_us|CPUQuotaPeriodSec|v1|进程执行耗用的CPU时间限额统计周期|
|cpu.cfs_quota_us|CPUQuota|v1|进程执行耗用的CPU时间限额|
|cpu.max|CPUQuota、CPUQuotaPeriodSec|v2|cgroup-v2合并了cpu.cfs_period_us和cpu.cfs_quota_us|
|cpu.shares|CPUShares（废弃）|v1|配置cgroup耗用的CPU时间权重|
|cpu.weight|CPUWeight|v2|配置cgroup耗用的CPU时间权重|
|cpu.idle|CPUWeight|v2|CPUWeight=idle标记cgroup为空闲调度|
|cpuset.cpus|AllowedCPUs|v2|cpuset子系统，允许使用的CPU|

### memory子系统接口

|接口名称|systemd配置|cgroup版本|说明|
|-|-|-|-|
|memory.limit_in_bytes|MemoryLimit（废弃）|v1|限制cgroup最大可使用的内存|
|memory.memsw.limit_in_bytes|MemoryMemswLimit|v1|限制cgroup最大可使用的内存、交换分区总和（openEuler自研，其他发行商不存在）|
|memory.max|MemoryMax、MemoryLimit|v2|限制cgroup可使用内存的硬限制，超过会触发OOM|
|memory.high|MemoryHigh|v2|限制cgroup可使用内存的软限制，实际使用的内存可以超过该值，但是超过后会触发内核积极回收内存，进程阻塞|
|memory.min|MemoryMin|v2|内存紧张时，保证cgroup最少可用内存的硬限制。内核在内存回收时，强制确保cgroup最少有memory.min的内存可用|
|memory.low|MemoryLow|v2|内存紧张时，保证cgroup最少可用内存的软限制。内核会尽量使cgroup最少有memory.low的内存可用，但不是强制性的，不会完全遵守|
|memory.swap.max|MemorySwapMax|v2|限制cgroup可使用的交换分区上限|
|memory.zswap.max|MemoryZSwapMax|v2|限制cgroup可使用的zswap上线|
|cpuset.mems|AllowedMemoryNodes|v2|cpuset子系统，允许使用的内存节点|

### PID子系统

|接口名称|systemd配置|cgroup版本|说明|
|-|-|-|-|
|pids.max|TasksMax|v1/v2|cgroup的进程上限|

### BlockIO/IO子系统接口

|接口名称|systemd配置|cgroup版本|说明|
|-|-|-|-|
|block.weight|BlockIOWeight（废弃）|v1|cgroup的块设备IO使用权重|
|blkio.weight_device|BlockIODeviceWeight（废弃）|v1|具体块设备的IO使用权重|
|blkio.throttle.read_bps_device|BlockIOReadBandwidth（废弃）|v1|具体块设备的读取带宽|
|blkio.throttle.write_bps_device|BlockIOWriteBandwidth（废弃）|v1|具体块设备的写入带宽|
|io.weight|IOWeight、BlockIOWeight、IODeviceWeight、BlockIODeviceWeight|v2|cgroup的全量块设备及具体块设备的IO使用权重|
|io.max|IOReadBandwidthMax、IOWriteBandwidthMax、BlockIOReadBandwidth、BlockIOWriteBandwidth|v2|cgroup的全量块设备及具体块设备的读写带宽|
|io.latency|IODeviceLatencyTargetSec|v2|cgroup具体块设备的IO延迟|

### cpuset and freezer

|接口名称|systemd配置|cgroup版本|说明|
|-|-|-|-|
|freezer.state|FreezerState|v1|cgroup冻结状态（openEuler自研，其他发行商不存在）|
|cpuset.cpus|CPUSetCpus|v1|允许使用的cpu（openEuler自研，其他发行商不存在）|
|cpuset.mems|CPUSetMems|v1|允许使用的内存节点（openEuler自研，其他发行商不存在）|
|cpuset.clone_children|CPUSetCloneChildren|v1|cpuset的继承行为（openEuler自研，其他发行商不存在）|
|cpuset.memory_migrate|CPUSetMemMigrate|v1|cgroup的内存迁移行为（openEuler自研，其他发行商不存在）|
|cpuset.cpus|AllowedCPUs|v2|允许使用的cpu|
|cpuset.mems|AllowedMemoryNodes|v2|允许使用的内存节点|
|cpuset.cpus.effective|EffectiveCPUs（只读）|v2|生效的cpu|
|cpuset.mems.effective|EffectiveMemoryNodes（只读）|v2|生效的内存节点|

#### systemd为什么不支持cgroup-v1的cpuset、freezer接口？

<https://github.com/systemd/systemd/pull/20580>

```
We communicated this at verious places. New features are only added to the cgroupsv2 logic since quite a while, cgroupsv1 is only supported to the level it is supported to right now.

In particular the two controllers you have been looking into are just broken in cgroupsv1, i.e. cpuset claims to be hierarchial, but is not, and freezer requires busy polling the state file to see when things completed.

Sorry. But this not the way forward: update to cgroupsv2 instead.

Sorry if this is disappointing.
```

### systemd用到的其他cgroup-v2接口

* pids.current，io.stat，memory.current，cpu.stat：systemd-cgtop用于展示cpu、io统计情况
* notify_on_release：监控cgroup的释放
* memory.swap.current，memory.stat：systemd-oomd特性使用
* cgroup.type：systemd-detect-virt特性使用
* cgroup.procs、cgroup.threads、cgroup.controllers、cgroup.events、cgroup.freeze、cgroup.subtree_control：systemd cgroup特性基础能力使用
* cgroup.stat：systemd-nspawn特性使用
* cgroup.max.depth、cgroup.max.descendants：systemd-255版本未使用，systemd TODO中表明后续会使用

### systemd废弃的resource-control接口

systemd-252版本废弃了部分cgroup-v1的`systemd.resource-control`接口。但是用户依旧可以使用这些接口，这些接口在cgroup-v2使用时，systemd会自动进行配置转换，具体的转换方法在下表的最后一列呈现。

|接口名称|作用|替换接口|systemd的v2兼容方案|
|-|-|-|-|
|CPUShares|配置cgroup-v1的cpu.shares，决定不同cgroup的CPU使用时间权重|CPUWeight|cpu.shares * 100 / 1024的值作为cpu.weight|
|StartupCPUShares|同上|StartupCPUWeigh|同上|
|MemoryLimit|配置cgroup-v1的memory.limit_in_bytes|MemoryMax|原始值配置到memory.max|
|BlockIOAccounting|启用单元的blkio子系统|NA|NA|
|BlockIOWeight|配置所有块设备的blkio.weight，blkio.bfq.weight|IOWeight|blkio.weight * 100 / 500的值作为io.weight|
|StartupBlockIOWeight|同上|StartupIOWeight|同上|
|BlockIODeviceWeight|配置特定块设备的blkio.weight，blkio.bfq.weight|IODeviceWeight|同BlockIOWeight|
|BlockIOReadBandwidth|配置特定块设备的读带宽blkio.throttle.read_bps_device|IOReadBandwidthMax|原始值配置到io.max的rbps|
|BlockIOWriteBandwidth|配置特定块设备的写带宽blkio.throttle.write_bps_device|IOWriteBandwidthMax|原始值配置到io.max的wbps|

## PSI

<https://www.cnblogs.com/Linux-tech/p/12961296.html>

全局的PSI值：`/proc/pressure`，per cgroup的PSI值：`/sys/fs/cgroup/{cpu.pressure, io.pressure, memory.pressure}`。

systemd的某些配置会读取这些配置文件，如：`ConditionMemoryPressure`, `ConditionCPUPressure`, `ConditionIOPressure`。systemd-oomd特性会额外读取：`memory.oom.group`，`memory.pressure`。

/proc/pressure/cpu的full行是没有意义的，在5.13内核前不存在该行。后来内核更新新增了该行，并将full行的内容设置为全0。

* `some`：至少有一个任务阻塞在某个资源上的时间占比。
* `full`：所有非idle任务都阻塞在某个资源上的时间占比，此时CPU资源将完全浪费。

每个CPU的PSI状态如下：

|PSI状态|描述|
|-|-|
|PSI_IO_SOME|至少存在一个任务处于iowait状态|
|PSI_IO_FULL|CPU上至少存在一个任务处于iowait状态，并且该CPU没有可执行的程序，进入了idle状态|
|PSI_MEM_SOME|类似，处于memory stall状态|
|PSI_MEM_FULL|类似|
|PSI_CPU_SOME|该CPU至少有一个任务在等待调度|
|PSI_NONIDLE|CPU并非处于完全空闲状态。即：没有任务在等待IO、等待MEM、或者CPU|

## 3 memcg

### 3.1 kernel Document

参考：<https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt>

#### 3.1.1 Design

The core of the design is a counter called the page_counter. The page_counter tracks the current memory usage and limit of the group of processes associated with the controller. Each cgroup has a memory controller specific data structure (mem_cgroup) associated with it.

#### 3.1.2 Accounting

```
                +--------------------+
                |  mem_cgroup        |
                |  (page_counter)    |
                +--------------------+
                 /            ^      \
                /             |       \
           +---------------+  |        +---------------+
           | mm_struct     |  |....    | mm_struct     |
           |               |  |        |               |
           +---------------+  |        +---------------+
                              |
                              + --------------+
                                              |
           +---------------+           +------+--------+
           | page          +---------->  page_cgroup   |
           |               |           |               |
           +---------------+           +---------------+
```

1. Accounting happens per cgroup
2. Each mm_struct knows about which cgroup it belongs to
3. Each page has a pointer to the page_cgroup, which in turn knows the cgroup it belongs to

The accounting is done as follows: mem_cgroup_charge_common() is invoked to set up the necessary data structures and check if the cgroup that is being charged is over its limit. If it is, then reclaim is invoked on the cgroup.
More details can be found in the reclaim section of this document. If everything goes well, a page meta-data-structure called page_cgroup is updated. page_cgroup has its own LRU on cgroup.

* page_cgroup structure is allocated at boot/memory-hotplug time.

#### 3.1.3 Accounting details

All mapped anon pages (RSS) and cache pages (Page Cache) are accounted.

Some pages which are never reclaimable and will not be on the LRU are not accounted. We just account pages under usual VM management.

RSS pages are accounted at page_fault unless they've already been accounted for earlier. A file page will be accounted for as Page Cache when it's inserted into inode (radix-tree). While it's mapped into the page tables of processes, duplicate accounting is carefully avoided.

An RSS page is unaccounted when it's fully unmapped. A PageCache page is unaccounted when it's removed from radix-tree. Even if RSS pages are fully unmapped (by kswapd), they may exist as SwapCache in the system until they are really freed. Such SwapCaches are also accounted.
A swapped-in page is not accounted until it's mapped.

Note: The kernel does swapin-readahead and reads multiple swaps at once.
This means swapped-in pages may contain pages for other tasks than a task causing page fault. So, we avoid accounting at swap-in I/O.

At page migration, accounting information is kept.

Note: we just account pages-on-LRU because our purpose is to control amount of used pages; not-on-LRU pages tend to be out-of-control from VM view.

#### 3.1.4 Shared Page Accounting

Shared pages are accounted on the basis of the first touch approach. The cgroup that first touches a page is accounted for the page. The principle behind this approach is that a cgroup that aggressively uses a shared page will eventually get charged for it (once it is uncharged from the cgroup that brought it in -- this will happen on memory pressure).

But see section 8.2: when moving a task to another cgroup, its pages may be recharged to the new cgroup, if move_charge_at_immigrate has been chosen.

Exception: If CONFIG_MEMCG_SWAP is not used. When you do swapoff and make swapped-out pages of shmem(tmpfs) to be backed into memory in force, charges for pages are accounted against the caller of swapoff rather than the users of shmem.

#### 3.1.5 Swap Extension (CONFIG_MEMCG_SWAP)

Swap Extension allows you to record charge for swap. A swapped-in page is charged back to original page allocator if possible.

When swap is accounted, following files are added.

* memory.memsw.usage_in_bytes.
* memory.memsw.limit_in_bytes.

memsw means memory+swap. Usage of memory+swap is limited by memsw.limit_in_bytes.

Example: Assume a system with 4G of swap. A task which allocates 6G of memory (by mistake) under 2G memory limitation will use all swap. In this case, setting memsw.limit_in_bytes=3G will prevent bad use of swap. By using the memsw limit, you can avoid system OOM which can be caused by swap shortage.

**why 'memory+swap' rather than swap.?**

The global LRU(kswapd) can swap out arbitrary pages. Swap-out means to move account from memory to swap...there is no change in usage of memory+swap. In other words, when we want to limit the usage of swap without affecting global LRU, memory+swap limit is better than just limiting swap from an OS point of view.

**What happens when a cgroup hits memory.memsw.limit_in_bytes?**

When a cgroup hits memory.memsw.limit_in_bytes, it's useless to do swap-out in this cgroup. Then, swap-out will not be done by cgroup routine and file caches are dropped. But as mentioned above, global LRU can do swapout memory from it for sanity of the system's memory management state. You can't forbid it by cgroup.

#### 3.1.6 Reclaim

Each cgroup maintains a per cgroup LRU which has the same structure as global VM. When a cgroup goes over its limit, we first try to reclaim memory from the cgroup so as to make space for the new pages that the cgroup has touched. If the reclaim is unsuccessful, an OOM routine is invoked to select and kill the bulkiest task in the cgroup. (See 10. OOM Control below.)

The reclaim algorithm has not been modified for cgroups, except that pages that are selected for reclaiming come from the per-cgroup LRU list.

NOTE: Reclaim does not work for the root cgroup, since we cannot set any limits on the root cgroup.

Note2: When panic_on_oom is set to "2", the whole system will panic.

When oom event notifier is registered, event will be delivered. (See oom_control section)

### 3.2 charge是什么东西？

参考：<https://www.kernel.org/doc/html/next/admin-guide/cgroup-v1/memcg_test.html>

核心代码：在do_anonymous_page、do_swap_page、do_cow_fault的时候会调用mem_cgroup_try_charge，将page charge到对应的memcg。

try_charge简单理解成记录的意思。try_charge()会调度到执行mem_cgroup_handle_over_high，此时，执行reclaim_high进行页面回收。
`try_charge() -> schedule_work(&memcg->high_work) -> (high_work.func = high_work_func) -> high_work_func -> reclaim_high`。注意这里和page_min有区别，此处是per cgroup的内存回收。

相关函数：mem_cgroup_try_charge, mem_cgroup_commit_charge, mem_cgroup_cancel_charge
